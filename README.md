## ThÃ´ng tin cÃ¡ nhÃ¢n
- **Há» vÃ  tÃªn**: Tráº§n Minh Äá»©c
- **MÃ£ sinh viÃªn (MSV)**: 23001518
- **Lá»›p**: 68KHDL  
- **NgÃ y sinh**: 09/12/2005  

---

## ğŸ“Œ Lab 1: Tokenization
### Ná»™i dung thá»±c hiá»‡n
- XÃ¢y dá»±ng **interface `Tokenizer`**.
- CÃ i Ä‘áº·t **SimpleTokenizer**:
  - Chuyá»ƒn chá»¯ thÆ°á»ng.
  - TÃ¡ch tá»« theo khoáº£ng tráº¯ng.
  - Xá»­ lÃ½ tÃ¡ch dáº¥u cÃ¢u Ä‘Æ¡n giáº£n (.,!?).
- CÃ i Ä‘áº·t **RegexTokenizer**:
  - Sá»­ dá»¥ng regex `\w+|[^\w\s]` Ä‘á»ƒ tÃ¡ch tá»« vÃ  dáº¥u cÃ¢u.
- Táº¡o `Lab1/main.py` Ä‘á»ƒ cháº¡y thá»­ trÃªn vÃ­ dá»¥ vÃ  dataset `UD_English-EWT`.

###  Há»c Ä‘Æ°á»£c
- Sá»± khÃ¡c biá»‡t giá»¯a **tokenizer thá»§ cÃ´ng** vÃ  **tokenizer regex**.


---

## ğŸ“Œ Lab 2: Vectorization
###  Ná»™i dung thá»±c hiá»‡n
- XÃ¢y dá»±ng **interface `Vectorizer`** vá»›i 3 phÆ°Æ¡ng thá»©c:
  - `fit(corpus)`
  - `transform(documents)`
  - `fit_transform(corpus)`
- CÃ i Ä‘áº·t **CountVectorizer**:
  - Nháº­n vÃ o má»™t `Tokenizer`.
  - Há»c **vocabulary** tá»« corpus.
  - Biáº¿n Ä‘á»•i vÄƒn báº£n thÃ nh **bag-of-words vector**.
- Táº¡o test (`test/lab2_test.py`) Ä‘á»ƒ cháº¡y vá»›i vÃ­ dá»¥.

### Há»c Ä‘Æ°á»£c
- CÃ¡ch cÃ i Ä‘áº·t thá»§ cÃ´ng mÃ´ hÃ¬nh Bag-of-Words.
- CÃ¡ch tÃ­ch há»£p tokenizer vÃ o vectorizer.

---

## âš™ï¸ CÃ¡ch cháº¡y
1. CÃ i Ä‘áº·t mÃ´i trÆ°á»ng:
   ```bash
   pip install -r requirements.txt
    ```
2. Cháº¡y test:
    - Lab 1:
    ```bash
    python -m test.lab1_test
    ```
    - Lab 2:
    ```bash
    python -m test.lab2_test  
    ```
